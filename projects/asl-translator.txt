Title:
ASL Gesture Translator

Description:
A computer vision project that detects your hand movements and signs for sign language. 

Project Metrics:

 - Purpose:
	I mainly built this prokect to learn Computer Vision, AI, and MediaPipe
 - Accuracy:
	97%
 - Data:
	All data that was used to train the model was made by me and my team

Body 1: <Title: Planning> <Asset: keypoint.csv, Layout: left>
Built a robust data collection and cleaning pipeline from live camera landmarks. MediaPipe extracts 21 hand keypoints per frame, feeding into a normalization layer that handles varying hand sizes and positions.

Body 2: <Title: Model Training>
Trained and evaluated MLP variants for recognition confidence and latency tradeoffs. The final architecture balances real-time inference speed with high accuracy across 26 ASL alphabet signs.

Body 3: <Title: Interaction Design>
Packaged inference outputs into a practical interaction loop for communication support. The system provides visual feedback, confidence indicators, and word-building capabilities.

Tools used:
Python, OpenCV, MediaPipe, MLP, Data Pipeline

Models:

